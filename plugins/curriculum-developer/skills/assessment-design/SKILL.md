# Assessment Design Skill

**Evidence-based practices for creating valid, reliable, and fair assessments**

This skill synthesizes assessment design principles from psychometrics, educational measurement, and decades of classroom practice to help create assessments that accurately measure learning.

---

## Core Principles

1. **Validity**: Assessment measures what it's supposed to measure
2. **Reliability**: Assessment produces consistent, dependable results
3. **Fairness**: Assessment is equitable and accessible to all learners
4. **Alignment**: Assessment matches learning objectives and instruction
5. **Utility**: Assessment results are useful for decision-making
6. **Transparency**: Criteria and expectations are clear to learners

---

## Assessment Fundamentals

### Types by Purpose

#### Formative Assessment (Assessment FOR Learning)

**Purpose**: Monitor progress, provide feedback, adjust instruction

**Characteristics**:
- Frequent, ongoing
- Low-stakes (minimal grade weight)
- Used during learning
- Informs next steps

**Examples**:
- Exit tickets
- Quick quizzes
- Drafts with feedback
- Practice problems
- Self-assessments
- Peer reviews

**Use Results To**:
- Identify struggling students early
- Adjust teaching approach
- Re-teach concepts
- Group students for targeted support

#### Summative Assessment (Assessment OF Learning)

**Purpose**: Evaluate achievement, assign grades

**Characteristics**:
- Infrequent (end of unit/course)
- High-stakes (major grade component)
- Used after learning
- Measures mastery

**Examples**:
- Unit tests
- Final exams
- Final projects
- Presentations
- Portfolios

**Use Results To**:
- Assign grades
- Determine proficiency
- Make placement decisions
- Evaluate program effectiveness

### Types by Format

#### Selected Response

**Formats**: Multiple choice, true/false, matching, fill-in-blank

**Strengths**:
- Objective scoring
- Efficient for large groups
- Can cover broad content
- Easy to analyze statistically

**Limitations**:
- Mainly tests recall/recognition
- Difficult to assess higher-order thinking
- Vulnerable to guessing
- Can be culturally biased

**Best For**: Knowledge and comprehension levels (Bloom's)

#### Constructed Response

**Formats**: Short answer, essay, problem-solving, show-work

**Strengths**:
- Assess deeper understanding
- See reasoning process
- Reduce guessing
- Can assess higher-order thinking

**Limitations**:
- Time-consuming to score
- Subjective without good rubric
- Can be influenced by writing ability
- Scoring reliability can be lower

**Best For**: Application, analysis, evaluation, creation (Bloom's)

#### Performance Assessment

**Formats**: Demonstrations, presentations, portfolios, projects, performances

**Strengths**:
- Authentic, real-world relevance
- Assess complex skills
- See process and product
- Engaging for learners

**Limitations**:
- Very time-consuming
- Requires detailed rubrics
- Can be expensive (resources)
- Harder to standardize

**Best For**: Complex skills, authentic application, creation level

---

## Alignment with Bloom's Taxonomy

**Critical**: Assessment type and cognitive level must match learning objective

### Remember/Understand (Lower Levels)

**Appropriate Assessments**:
- Multiple choice
- Matching
- True/false
- Short answer (define, list, describe)
- Labeling diagrams

**Example Items**:
- "Define photosynthesis"
- "List three causes of WWI"
- "Which of the following is an example of a renewable resource?"

### Apply

**Appropriate Assessments**:
- Problem sets
- Case applications
- Demonstrations
- Simulations

**Example Items**:
- "Calculate the area of this irregular shape"
- "Use the quadratic formula to solve..."
- "Demonstrate proper hand-washing technique"

### Analyze

**Appropriate Assessments**:
- Compare/contrast essays
- Case analysis
- Critiques
- Data interpretation

**Example Items**:
- "Compare the themes in these two novels"
- "Analyze the causes of this historical event"
- "What patterns do you see in this data set?"

### Evaluate

**Appropriate Assessments**:
- Position papers
- Critiques
- Debates
- Reviews

**Example Items**:
- "Evaluate the effectiveness of this policy"
- "Critique the methodology of this study"
- "Which solution is best and why?"

### Create

**Appropriate Assessments**:
- Original projects
- Designs
- Research papers
- Performances
- Business plans

**Example Items**:
- "Design an experiment to test this hypothesis"
- "Create a marketing plan for this product"
- "Compose an original piece in the Baroque style"

---

## Multiple Choice Best Practices

### Anatomy of a Quality Multiple Choice Item

**Components**:
1. **Stem**: Question or incomplete statement
2. **Correct Answer**: The one best answer
3. **Distractors**: Plausible but incorrect options

### Writing Effective Stems

**✅ Do**:
- Pose clear, focused question
- Include as much of item as possible in stem
- Make stem a complete thought or question
- Use positive phrasing when possible
- Test important concepts (not trivial details)

**❌ Don't**:
- Use "all of the above" or "none of the above"
- Use double negatives
- Include irrelevant information
- Make stem too long or complex
- Test memorization of insignificant facts

**Examples**:

❌ **Poor Stem**:
```
Which of the following is true?
A. Mitosis has 4 stages
B. Meiosis produces gametes
C. Both A and B
D. None of the above
```
*Problem*: Vague, uses "all/none of above"

✅ **Better Stem**:
```
What is the primary purpose of meiosis?
A. Cell growth
B. Tissue repair
C. Production of gametes
D. Asexual reproduction
```
*Why better*: Clear question, focused, no "all/none"

### Writing Effective Distractors

**Characteristics of Good Distractors**:
- Plausible to students who haven't mastered content
- Not obviously wrong
- Similar length/complexity to correct answer
- Parallel grammatical structure
- Based on common misconceptions

**✅ Do**:
- Make all options similar length
- Use parallel grammar
- Base on actual student errors
- Make all options plausible

**❌ Don't**:
- Include clues (e.g., "an" before only vowel option)
- Use "always" or "never" (makes obviously wrong)
- Make correct answer consistently longer
- Include overlapping options

**Example**:

❌ **Poor Distractors**:
```
Photosynthesis produces:
A. Water
B. Oxygen and glucose through the process of converting light energy
C. Carbon dioxide
D. Nothing
```
*Problems*: B is much longer (clue), D is absurd, options aren't parallel

✅ **Better Distractors**:
```
Photosynthesis produces:
A. Water and oxygen
B. Glucose and oxygen
C. Carbon dioxide and water
D. Glucose and carbon dioxide
```
*Why better*: All plausible, similar length, parallel structure

### Avoiding Common MC Flaws

**Flaw 1: Grammatical Clues**

❌ **Problem**:
```
An example of a mammal is an:
A. Shark
B. Eagle
C. Octopus
D. Elephant
```
*"an" before options makes D obviously correct*

✅ **Fix**:
```
Which of the following is a mammal?
[Same options]
```

**Flaw 2: Absolute Language**

❌ **Problem**:
```
Economic growth always leads to:
A. Inflation
B. Higher employment
C. Reduced poverty
D. Environmental damage
```
*"Always" makes all options suspicious*

✅ **Fix**:
```
Economic growth typically leads to:
[Options with "may increase", "often leads to", etc.]
```

**Flaw 3: Overlapping Options**

❌ **Problem**:
```
The primary cause of the American Civil War was:
A. Slavery
B. Economic differences between North and South
C. States' rights
D. All of the above
```
*A is a subset of B and C; overlapping*

✅ **Fix**: Make options mutually exclusive

**Flaw 4: Pattern in Correct Answers**

❌ **Problem**: Correct answer is always C

✅ **Fix**: Randomize position of correct answer

### Cognitive Level in MC Items

**Remember Level MC**:
```
What is the capital of France?
A. London
B. Paris
C. Berlin
D. Madrid
```

**Understand Level MC**:
```
Why do coastal areas have milder climates than inland areas?
A. They are always windy
B. Water heats and cools more slowly than land
C. They receive more rainfall
D. They are at lower elevations
```

**Apply Level MC**:
```
A ball is thrown straight up at 20 m/s. How long until it returns to the
thrower's hand? (g = 10 m/s²)
A. 2 seconds
B. 4 seconds
C. 6 seconds
D. 8 seconds
```

**Analyze Level MC**:
```
Based on the graph showing temperature and precipitation, which biome is most
likely represented?
A. Tropical rainforest
B. Desert
C. Temperate deciduous forest
D. Tundra
```

---

## Essay and Constructed Response

### Writing Effective Essay Prompts

**Components of Good Essay Prompt**:
1. Clear task
2. Specific requirements
3. Context or scenario (if needed)
4. Expected length/scope
5. Evaluation criteria

**Template**:
```
[Context or scenario if needed]

[Clear task statement with action verb]

Your response should:
- [Specific requirement 1]
- [Specific requirement 2]
- [Specific requirement 3]

Length: [Word count or page range]

You will be evaluated on: [Key criteria]
```

**Examples by Bloom's Level**:

**Understand Level**:
```
Explain the process of natural selection in your own words.

Your explanation should:
- Define natural selection
- Describe the key conditions required
- Provide at least one example
- Use clear, precise language

Length: 250-300 words
```

**Analyze Level**:
```
Compare and contrast the American and French Revolutions.

Your analysis should:
- Identify at least 3 similarities
- Identify at least 3 differences
- Explain causes of these similarities and differences
- Support claims with historical evidence

Length: 500-600 words
```

**Evaluate Level**:
```
Read the two proposals for addressing climate change. Evaluate which proposal
is more effective and why.

Your evaluation should:
- Identify criteria for effectiveness (feasibility, impact, cost, etc.)
- Assess each proposal against these criteria
- Make and defend a recommendation
- Address potential counterarguments

Length: 600-700 words
```

**Create Level**:
```
Design an experiment to test the hypothesis that plants grow faster with
classical music.

Your experimental design should include:
- Clear statement of hypothesis
- Identification of variables (independent, dependent, controlled)
- Description of procedure
- Explanation of how results would support or refute hypothesis
- Discussion of potential limitations

Length: 400-500 words
```

### Avoiding Vague Prompts

❌ **Too Vague**:
```
Write about World War II.
```
*Problem*: No focus, no criteria, overwhelming scope

✅ **Specific**:
```
Analyze the role of technology in determining the outcome of World War II.
Focus on at least two specific technologies and their strategic impact.
```

❌ **Too Vague**:
```
Discuss the themes in Hamlet.
```
*Problem*: Which themes? How many? How deep?

✅ **Specific**:
```
Analyze how Shakespeare develops the theme of appearance vs. reality in Hamlet.
Use at least three specific scenes as evidence.
```

---

## Rubric Design

### Types of Rubrics

#### Analytic Rubric
**Structure**: Separate scoring for each criterion

**Use When**:
- Detailed feedback needed
- Multiple distinct criteria
- Want to identify specific strengths/weaknesses
- Formative assessment

**Example Structure**:
| Criterion | Exemplary (4) | Proficient (3) | Developing (2) | Beginning (1) |
|-----------|---------------|----------------|----------------|---------------|
| Content   | ...           | ...            | ...            | ...           |
| Analysis  | ...           | ...            | ...            | ...           |
| Evidence  | ...           | ...            | ...            | ...           |
| Writing   | ...           | ...            | ...            | ...           |

**Advantages**:
- Specific, actionable feedback
- Clear what to improve
- Fair (compensates strong area for weak)

**Disadvantages**:
- Takes longer to score
- May be overly detailed for some tasks

#### Holistic Rubric
**Structure**: Single overall score

**Use When**:
- Quick scoring needed
- Overall impression matters
- Summative assessment
- Consistent pattern across criteria

**Example Structure**:
- **Exemplary (4)**: Exceptional work demonstrating mastery...
- **Proficient (3)**: Good work meeting expectations...
- **Developing (2)**: Acceptable work with gaps...
- **Beginning (1)**: Work showing little understanding...

**Advantages**:
- Fast to score
- Captures overall quality
- Good for large-scale assessment

**Disadvantages**:
- Less specific feedback
- Harder to identify what to improve

### Writing Effective Performance Levels

**Characteristics of Good Descriptors**:
- Observable, concrete behaviors
- Clear distinctions between levels
- Positive language (what IS present, not absent)
- Specific examples when possible
- Avoid vague terms (good, poor, adequate)

**Number of Levels**:
- 3 levels: Basic, Proficient, Advanced (minimum)
- 4 levels: Beginning, Developing, Proficient, Exemplary (most common)
- 5-6 levels: For finer discrimination (e.g., grading scale)

**Writing Process**:
1. Define proficient first (meets expectations)
2. Define highest level (exceeds expectations)
3. Define lowest level (does not meet)
4. Fill in middle level(s)

**Example: Research Paper Thesis**

**Exemplary (4)**:
Thesis is clearly stated, specific, arguable, and insightful. It makes a
sophisticated claim that goes beyond obvious interpretations and previews
the structure of the argument.

**Proficient (3)**:
Thesis is clearly stated, specific, and arguable. It makes a clear claim that
guides the paper's argument.

**Developing (2)**:
Thesis is present but may be vague, too broad, or not fully arguable. The
claim is unclear or obvious.

**Beginning (1)**:
Thesis is absent, unclear, or is a statement of fact rather than an arguable
claim.

### Common Rubric Criteria by Subject

**Writing**:
- Thesis/main idea
- Evidence and support
- Organization
- Analysis/critical thinking
- Style and voice
- Conventions (grammar, mechanics)

**Oral Presentation**:
- Content and accuracy
- Organization and structure
- Delivery (eye contact, voice, pace)
- Visual aids (if applicable)
- Response to questions
- Time management

**Scientific Investigation**:
- Research question/hypothesis
- Methodology and procedure
- Data collection and recording
- Analysis and interpretation
- Conclusions
- Communication of findings

**Problem Solving**:
- Problem identification
- Strategy selection
- Execution of solution
- Verification of answer
- Communication of reasoning

### Avoiding Rubric Pitfalls

**Pitfall 1: Vague Language**

❌ **Too Vague**:
"Shows good understanding of the topic"
*What does "good" mean? How would you recognize it?*

✅ **Specific**:
"Accurately explains all key concepts and provides relevant examples for each"

**Pitfall 2: Counting Instead of Quality**

❌ **Focuses on Quantity**:
"Includes 5 or more sources"
*More isn't always better*

✅ **Focuses on Quality**:
"Uses credible, relevant sources that strongly support the argument"

**Pitfall 3: Negative Language**

❌ **Negative**:
"Contains many errors, lacks organization, does not support claims"

✅ **Positive**:
"Demonstrates consistent accuracy, clear organization, and well-supported claims"

**Pitfall 4: Overlapping Levels**

❌ **Unclear Distinctions**:
- Proficient: "3-4 sources"
- Developing: "2-3 sources"
*Overlap at 3 sources*

✅ **Clear Distinctions**:
- Proficient: "4 or more sources"
- Developing: "2-3 sources"

---

## Performance Assessment Design

### Authentic Assessment

**Characteristics**:
- Real-world context and relevance
- Complex, multifaceted challenges
- Sustained effort over time
- Multiple acceptable solutions
- Requires judgment and innovation

**Examples**:
- Design a solution to a community problem
- Develop a business plan
- Create a portfolio demonstrating growth
- Conduct original research
- Perform a skill in authentic context

### Designing Performance Tasks

**Components**:
1. **Scenario/Context**: Realistic situation
2. **Task**: What students must produce or perform
3. **Audience**: Who the work is for (beyond teacher)
4. **Standards**: Criteria for success

**Template**:
```
**Scenario**: [Realistic context]

**Your Role**: [Who are you in this scenario]

**Your Task**: [What you must create/do]

**Audience**: [Who will use/view your work]

**Requirements**:
- [Specific requirement 1]
- [Specific requirement 2]
- [Specific requirement 3]

**Deliverables**:
- [Product 1]
- [Product 2]

**Success Criteria**: [Link to rubric]

**Timeline**: [Due dates for milestones]
```

**Example: Environmental Science**

```
**Scenario**:
Your city is experiencing water shortages and needs to develop a conservation
plan.

**Your Role**:
Environmental consultant hired by the city council

**Your Task**:
Develop a comprehensive water conservation plan that is scientifically sound,
economically feasible, and publicly acceptable.

**Audience**:
City council members who will vote on your proposal

**Requirements**:
- Research current water usage and projections
- Identify at least 5 conservation strategies
- Analyze cost and impact of each strategy
- Recommend priorities and implementation timeline
- Address potential public concerns

**Deliverables**:
- Written proposal (10-12 pages)
- Presentation to "city council" (10 minutes)
- Visual aids (slides, infographics, etc.)

**Evaluation**: See rubric (assessing scientific accuracy, feasibility,
communication, and creativity)

**Timeline**:
- Week 1: Research submitted
- Week 2: Draft proposal peer review
- Week 3: Final proposal and presentation
```

### Portfolio Assessment

**Types**:
- **Process Portfolio**: Shows development over time
- **Product Portfolio**: Best work samples
- **Showcase Portfolio**: Selected pieces for specific audience

**Components**:
- Table of contents
- Introduction/overview
- Selected works with captions
- Reflections on each piece
- Self-assessment of growth

**Evaluation Criteria**:
- Quality of individual pieces
- Evidence of growth/improvement
- Range of skills demonstrated
- Depth of reflection
- Organization and presentation

---

## Assessment Quality Assurance

### Ensuring Validity

**Content Validity**: Does assessment cover the content domain?
- Create test blueprint/table of specifications
- Map items to objectives
- Ensure representative sampling

**Example Test Blueprint**:
| Objective | Remember | Understand | Apply | Total Items | % of Test |
|-----------|----------|------------|-------|-------------|-----------|
| Obj 1     | 2        | 2          | 1     | 5           | 25%       |
| Obj 2     | 1        | 3          | 2     | 6           | 30%       |
| Obj 3     | 2        | 2          | 0     | 4           | 20%       |
| Obj 4     | 1        | 3          | 1     | 5           | 25%       |
| **Total** | **6**    | **10**     | **4** | **20**      | **100%**  |

**Construct Validity**: Does it measure the intended construct?
- Align with Bloom's level
- Avoid construct-irrelevant difficulty (e.g., reading level too high)
- Pilot test with similar students

### Ensuring Reliability

**Consistency of Measurement**

**Strategies**:
- Clear, unambiguous items
- Detailed rubrics for subjective scoring
- Multiple scorers (inter-rater reliability)
- Adequate number of items per objective (3-5 items minimum)
- Consistent testing conditions

**Rubric Reliability**:
- Train all scorers using rubric
- Score sample together to calibrate
- Check agreement rate (should be >80%)
- Discuss discrepancies
- Refine rubric as needed

### Ensuring Fairness

**Avoid Bias**:
- Review for cultural specificity (references only some students know)
- Avoid stereotypes
- Use diverse names and contexts
- Ensure language is accessible
- Avoid assumptions about background knowledge

**Provide Accommodations**:
- Extended time for students who need it
- Assistive technology
- Alternative formats
- Quiet testing space
- Clarification of directions

**Universal Design for Assessment**:
- Multiple formats for demonstrating knowledge
- Clear, simple language
- Consistent format and layout
- Reduced visual clutter
- Accessible technology

---

## Item Analysis and Improvement

### Analyzing Multiple Choice Items

**Item Difficulty**: Percentage who answered correctly
- **Too Easy** (>90%): Not discriminating, may be too obvious
- **Appropriate** (40-80%): Good range
- **Too Hard** (<30%): May be too difficult or poorly written

**Item Discrimination**: Do high performers answer correctly more often than low performers?
- Positive discrimination: Good item
- Zero or negative: Problem - revise or remove

**Distractor Analysis**: Which wrong answers are chosen?
- If distractor never chosen: Replace with more plausible option
- If distractor chosen more than correct answer: Review item for clarity

### Pilot Testing

**Process**:
1. Administer to small sample (similar to target)
2. Collect data on each item
3. Analyze difficulty, discrimination, distractors
4. Interview some students: Why did you choose this answer?
5. Revise problematic items
6. Re-test if major changes

### Common Item Flaws and Fixes

**Flaw**: Item too hard (everyone misses)
**Diagnosis**: Check for ambiguous wording, trick phrasing, unfamiliar context
**Fix**: Clarify stem, improve distractors, test more accessible content

**Flaw**: Item too easy (everyone gets it)
**Diagnosis**: Clues in item, obviously wrong distractors, trivial content
**Fix**: Improve distractors, remove clues, test at higher cognitive level

**Flaw**: Negative discrimination (low performers do better)
**Diagnosis**: Ambiguous wording, correct answer is wrong, high performers overthink
**Fix**: Review correct answer, clarify wording, simplify

---

## Best Practices Summary

### Assessment Design Checklist

**Planning**:
- [ ] Learning objectives clearly defined
- [ ] Assessment type matches objective's cognitive level
- [ ] Blueprint or specification table created
- [ ] Adequate sampling of content
- [ ] Formative and summative balance

**Item Writing**:
- [ ] Clear, unambiguous language
- [ ] Free from bias and cultural assumptions
- [ ] Appropriate difficulty for audience
- [ ] All options grammatically parallel (MC)
- [ ] Rubrics detailed and clear (essays, performance)

**Administration**:
- [ ] Clear instructions provided
- [ ] Adequate time allocated
- [ ] Accommodations arranged
- [ ] Consistent conditions for all

**Scoring**:
- [ ] Answer key/rubric prepared in advance
- [ ] Scoring procedures consistent
- [ ] Feedback timely and specific
- [ ] Results used to improve learning and teaching

**Review and Improvement**:
- [ ] Items analyzed (difficulty, discrimination)
- [ ] Student feedback gathered
- [ ] Problematic items revised
- [ ] Assessment effectiveness evaluated

### Red Flags - Items That Need Revision

- ❌ No one answers correctly (or everyone does)
- ❌ Low performers do better than high performers
- ❌ Students complain item is unclear or unfair
- ❌ Correct answer is debatable
- ❌ Item assesses trivial information
- ❌ Item requires outside knowledge not taught
- ❌ Item has multiple defensible answers
- ❌ Cultural or linguistic bias evident

### The Golden Rule of Assessment

**If you can't explain how the assessment result will be used to make a decision or improve learning, don't give the assessment.**

Assessment should:
- Inform instructional decisions
- Provide feedback to learners
- Measure achievement of objectives
- Motivate and guide learning

Assessment should NOT:
- Trick or trap students
- Cover what wasn't taught
- Serve as punishment
- Be given "just because"

---

**Use these principles to create assessments that are fair, valid, reliable, and truly measure what students have learned.**
